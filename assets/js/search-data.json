{
  
    
        "post0": {
            "title": "The US Accidents Exploratory Data Analysis",
            "content": "Importing the nescessary Libraries . import pandas as pd # For manipulating Tabular Data import numpy as np # For fast operations on array like data structures import matplotlib.pyplot as plt # For plotting and grpahs import seaborn as sns # A library built on top of Matplotlib also used for plotting import folium # For plotting geographical plots from folium import plugins import plotly.graph_objects as go # For plotting graphs with plotly . Reading in the Data . For reading files in we use the Pandas library which provides easy, fast and reliable way to read CSV (Comma Seperated Values) and other formats. . df = pd.read_csv(&#39;D://Datasets/US_Accidents.csv&#39;) # This function creates a Tabular structure called as DataFrame for us which facilitates an easy manipulation of rows and Columns. . Setting display so that the DataFrame shows all the columns in the CSV file . This line of code here is a pandas function which allows us to display at maximum the number of columns on the right becuse sometimes when there are too many columns JUPYTER trims the contents of the DataFrame fit the whole DataFrame in the display. . pd.options.display.max_columns = 50 . Displaying the the DataFrame . df . ID Severity Start_Time End_Time Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Description Number Street Side City County State Zipcode Country Timezone Airport_Code Weather_Timestamp Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Direction Wind_Speed(mph) Precipitation(in) Weather_Condition Amenity Bump Crossing Give_Way Junction No_Exit Railway Roundabout Station Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset Civil_Twilight Nautical_Twilight Astronomical_Twilight . 0 A-2716600 | 3 | 2016-02-08 00:37:08 | 2016-02-08 06:37:08 | 40.10891 | -83.09286 | 40.11206 | -83.03187 | 3.230 | Between Sawmill Rd/Exit 20 and OH-315/Olentang... | NaN | Outerbelt E | R | Dublin | Franklin | OH | 43017 | US | US/Eastern | KOSU | 2016-02-08 00:53:00 | 42.1 | 36.1 | 58.0 | 29.76 | 10.0 | SW | 10.4 | 0.00 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 1 A-2716601 | 2 | 2016-02-08 05:56:20 | 2016-02-08 11:56:20 | 39.86542 | -84.06280 | 39.86501 | -84.04873 | 0.747 | At OH-4/OH-235/Exit 41 - Accident. | NaN | I-70 E | R | Dayton | Montgomery | OH | 45424 | US | US/Eastern | KFFO | 2016-02-08 05:58:00 | 36.9 | NaN | 91.0 | 29.68 | 10.0 | Calm | NaN | 0.02 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 2 A-2716602 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10266 | -84.52468 | 39.10209 | -84.52396 | 0.055 | At I-71/US-50/Exit 1 - Accident. | NaN | I-75 S | R | Cincinnati | Hamilton | OH | 45203 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 3 A-2716603 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10148 | -84.52341 | 39.09841 | -84.52241 | 0.219 | At I-71/US-50/Exit 1 - Accident. | NaN | US-50 E | R | Cincinnati | Hamilton | OH | 45202 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 4 A-2716604 | 2 | 2016-02-08 06:51:45 | 2016-02-08 12:51:45 | 41.06213 | -81.53784 | 41.06217 | -81.53547 | 0.123 | At Dart Ave/Exit 21 - Accident. | NaN | I-77 N | R | Akron | Summit | OH | 44311 | US | US/Eastern | KAKR | 2016-02-08 06:54:00 | 39.0 | NaN | 55.0 | 29.65 | 10.0 | Calm | NaN | NaN | Overcast | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Day | Day | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1516059 A-4239402 | 2 | 2019-08-23 18:03:25 | 2019-08-23 18:32:01 | 34.00248 | -117.37936 | 33.99888 | -117.37094 | 0.543 | At Market St - Accident. | NaN | Pomona Fwy E | R | Riverside | Riverside | CA | 92501 | US | US/Pacific | KRAL | 2019-08-23 17:53:00 | 86.0 | 86.0 | 40.0 | 28.92 | 10.0 | W | 13.0 | 0.00 | Fair | False | False | False | False | False | False | False | False | False | False | False | False | False | Day | Day | Day | Day | . 1516060 A-4239403 | 2 | 2019-08-23 19:11:30 | 2019-08-23 19:38:23 | 32.76696 | -117.14806 | 32.76555 | -117.15363 | 0.338 | At Camino Del Rio/Mission Center Rd - Accident. | NaN | I-8 W | R | San Diego | San Diego | CA | 92108 | US | US/Pacific | KMYF | 2019-08-23 18:53:00 | 70.0 | 70.0 | 73.0 | 29.39 | 10.0 | SW | 6.0 | 0.00 | Fair | False | False | False | False | False | False | False | False | False | False | False | False | False | Day | Day | Day | Day | . 1516061 A-4239404 | 2 | 2019-08-23 19:00:21 | 2019-08-23 19:28:49 | 33.77545 | -117.84779 | 33.77740 | -117.85727 | 0.561 | At Glassell St/Grand Ave - Accident. in the ri... | NaN | Garden Grove Fwy | R | Orange | Orange | CA | 92866 | US | US/Pacific | KSNA | 2019-08-23 18:53:00 | 73.0 | 73.0 | 64.0 | 29.74 | 10.0 | SSW | 10.0 | 0.00 | Partly Cloudy | False | False | False | False | True | False | False | False | False | False | False | False | False | Day | Day | Day | Day | . 1516062 A-4239405 | 2 | 2019-08-23 19:00:21 | 2019-08-23 19:29:42 | 33.99246 | -118.40302 | 33.98311 | -118.39565 | 0.772 | At CA-90/Marina Fwy/Jefferson Blvd - Accident. | NaN | San Diego Fwy S | R | Culver City | Los Angeles | CA | 90230 | US | US/Pacific | KSMO | 2019-08-23 18:51:00 | 71.0 | 71.0 | 81.0 | 29.62 | 10.0 | SW | 8.0 | 0.00 | Fair | False | False | False | False | False | False | False | False | False | False | False | False | False | Day | Day | Day | Day | . 1516063 A-4239406 | 2 | 2019-08-23 18:52:06 | 2019-08-23 19:21:31 | 34.13393 | -117.23092 | 34.13736 | -117.23934 | 0.537 | At Highland Ave/Arden Ave - Accident. | NaN | CA-210 W | R | Highland | San Bernardino | CA | 92346 | US | US/Pacific | KSBD | 2019-08-23 20:50:00 | 79.0 | 79.0 | 47.0 | 28.63 | 7.0 | SW | 7.0 | 0.00 | Fair | False | False | False | False | False | False | False | False | False | False | False | False | False | Day | Day | Day | Day | . 1516064 rows Ã— 47 columns . The next snippet of code provides a small information about the columns such as datatypes of the Columns, Number of Non-Null values in the Columns etc. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1516064 entries, 0 to 1516063 Data columns (total 47 columns): # Column Non-Null Count Dtype -- -- 0 ID 1516064 non-null object 1 Severity 1516064 non-null int64 2 Start_Time 1516064 non-null object 3 End_Time 1516064 non-null object 4 Start_Lat 1516064 non-null float64 5 Start_Lng 1516064 non-null float64 6 End_Lat 1516064 non-null float64 7 End_Lng 1516064 non-null float64 8 Distance(mi) 1516064 non-null float64 9 Description 1516064 non-null object 10 Number 469969 non-null float64 11 Street 1516064 non-null object 12 Side 1516064 non-null object 13 City 1515981 non-null object 14 County 1516064 non-null object 15 State 1516064 non-null object 16 Zipcode 1515129 non-null object 17 Country 1516064 non-null object 18 Timezone 1513762 non-null object 19 Airport_Code 1511816 non-null object 20 Weather_Timestamp 1485800 non-null object 21 Temperature(F) 1473031 non-null float64 22 Wind_Chill(F) 1066748 non-null float64 23 Humidity(%) 1470555 non-null float64 24 Pressure(in) 1479790 non-null float64 25 Visibility(mi) 1471853 non-null float64 26 Wind_Direction 1474206 non-null object 27 Wind_Speed(mph) 1387202 non-null float64 28 Precipitation(in) 1005515 non-null float64 29 Weather_Condition 1472057 non-null object 30 Amenity 1516064 non-null bool 31 Bump 1516064 non-null bool 32 Crossing 1516064 non-null bool 33 Give_Way 1516064 non-null bool 34 Junction 1516064 non-null bool 35 No_Exit 1516064 non-null bool 36 Railway 1516064 non-null bool 37 Roundabout 1516064 non-null bool 38 Station 1516064 non-null bool 39 Stop 1516064 non-null bool 40 Traffic_Calming 1516064 non-null bool 41 Traffic_Signal 1516064 non-null bool 42 Turning_Loop 1516064 non-null bool 43 Sunrise_Sunset 1515981 non-null object 44 Civil_Twilight 1515981 non-null object 45 Nautical_Twilight 1515981 non-null object 46 Astronomical_Twilight 1515981 non-null object dtypes: bool(13), float64(13), int64(1), object(20) memory usage: 412.1+ MB . We can see that some of the columns have NUll values. Lets plot the percentage the Null values because it might come handy for later analysis. . missing_values = df.isna().sum().sort_values(ascending = False) missing_percentages = missing_values/ len(df) missing_percentages = missing_percentages[missing_percentages != 0] # just taking the columns whose percentage of Null values is not equal to zero. missing_percentages . Number 0.690007 Precipitation(in) 0.336760 Wind_Chill(F) 0.296370 Wind_Speed(mph) 0.084998 Humidity(%) 0.030018 Visibility(mi) 0.029162 Weather_Condition 0.029027 Temperature(F) 0.028385 Wind_Direction 0.027610 Pressure(in) 0.023926 Weather_Timestamp 0.019962 Airport_Code 0.002802 Timezone 0.001518 Zipcode 0.000617 Sunrise_Sunset 0.000055 Civil_Twilight 0.000055 Nautical_Twilight 0.000055 Astronomical_Twilight 0.000055 City 0.000055 dtype: float64 . the .isna() method just tells us that the value in a row or column is NULL or not (True/False) and the .sum() just sums up the number of True&#39;s (treats Ture&#39;s as 1 and False as 0) and the .sort_values(ascending = False) sorts the values in desending order and gives out the result. . Plotting the percentage of missing values . sns.set_style(&#39;darkgrid&#39;) # Sets the style of the plots to a predefined style which is handeled by the seaborn package plt.figure(figsize = (10,8)) # Sets the total size of the figure on which we would plot the contents. plt.title(&#39;Percentage of missing Values by column&#39;, weight = &#39;bold&#39;) # Sets the title of the plot plt.xlabel(&#39;Percentage of Missing Values&#39;,weight = &#39;bold&#39;) # Sets the label text to be displayed at the x-axis plt.xticks(np.arange(0,1, 0.05)) # Sets the tick values of the x-axis. Can be chaged for the y axis plt.ylabel(&#39;Columns&#39; ,weight = &#39;bold&#39;) # Sets the label text to be displayed at the y-axis sns.barplot(x = missing_percentages.values, y = missing_percentages.index) # Plots a barplotof the values plt.show() # Shows the figure (This is an optional step as JUPYTER automatically renders the figure in the output) . Summary Statistics . Now lets have a quick look at the summary statistics of the numerical columns. The DataFrame.describe() function provides us the summary statistics of all the numerical columns in the DataFrame. . df.describe().T # The .T in df.describe().T just Transposes the columns and rows to increase the readability. . count mean std min 25% 50% 75% max . Severity 1516064.0 | 2.238630 | 0.608148 | 1.000000 | 2.000000 | 2.000000 | 2.000000 | 4.000000e+00 | . Start_Lat 1516064.0 | 36.900558 | 5.165653 | 24.570222 | 33.854225 | 37.351130 | 40.725927 | 4.900058e+01 | . Start_Lng 1516064.0 | -98.599194 | 18.496022 | -124.497567 | -118.207575 | -94.381003 | -80.874690 | -6.711317e+01 | . End_Lat 1516064.0 | 36.900606 | 5.165629 | 24.570110 | 33.854204 | 37.351342 | 40.725930 | 4.907500e+01 | . End_Lng 1516064.0 | -98.599010 | 18.495903 | -124.497829 | -118.207746 | -94.379875 | -80.874490 | -6.710924e+01 | . Distance(mi) 1516064.0 | 0.587262 | 1.632659 | 0.000000 | 0.000000 | 0.178000 | 0.594000 | 1.551860e+02 | . Number 469969.0 | 8907.533114 | 22421.896710 | 0.000000 | 1212.000000 | 4000.000000 | 10100.000000 | 9.999997e+06 | . Temperature(F) 1473031.0 | 59.584597 | 18.273164 | -89.000000 | 47.000000 | 61.000000 | 73.000000 | 1.706000e+02 | . Wind_Chill(F) 1066748.0 | 55.109760 | 21.127345 | -89.000000 | 40.800000 | 57.000000 | 71.000000 | 1.130000e+02 | . Humidity(%) 1470555.0 | 64.659601 | 23.259865 | 1.000000 | 48.000000 | 68.000000 | 84.000000 | 1.000000e+02 | . Pressure(in) 1479790.0 | 29.554954 | 1.016756 | 0.000000 | 29.440000 | 29.880000 | 30.040000 | 5.804000e+01 | . Visibility(mi) 1471853.0 | 9.131755 | 2.889112 | 0.000000 | 10.000000 | 10.000000 | 10.000000 | 1.400000e+02 | . Wind_Speed(mph) 1387202.0 | 7.630812 | 5.637364 | 0.000000 | 4.600000 | 7.000000 | 10.400000 | 9.840000e+02 | . Precipitation(in) 1005515.0 | 0.008478 | 0.129317 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 2.400000e+01 | . Exploratory Data Analysis and Visualization . The List of the columns in the DataFrame are : . df.columns # The .columns() method ouputs a List of all the columns in the DataFrame. . Index([&#39;ID&#39;, &#39;Severity&#39;, &#39;Start_Time&#39;, &#39;End_Time&#39;, &#39;Start_Lat&#39;, &#39;Start_Lng&#39;, &#39;End_Lat&#39;, &#39;End_Lng&#39;, &#39;Distance(mi)&#39;, &#39;Description&#39;, &#39;Number&#39;, &#39;Street&#39;, &#39;Side&#39;, &#39;City&#39;, &#39;County&#39;, &#39;State&#39;, &#39;Zipcode&#39;, &#39;Country&#39;, &#39;Timezone&#39;, &#39;Airport_Code&#39;, &#39;Weather_Timestamp&#39;, &#39;Temperature(F)&#39;, &#39;Wind_Chill(F)&#39;, &#39;Humidity(%)&#39;, &#39;Pressure(in)&#39;, &#39;Visibility(mi)&#39;, &#39;Wind_Direction&#39;, &#39;Wind_Speed(mph)&#39;, &#39;Precipitation(in)&#39;, &#39;Weather_Condition&#39;, &#39;Amenity&#39;, &#39;Bump&#39;, &#39;Crossing&#39;, &#39;Give_Way&#39;, &#39;Junction&#39;, &#39;No_Exit&#39;, &#39;Railway&#39;, &#39;Roundabout&#39;, &#39;Station&#39;, &#39;Stop&#39;, &#39;Traffic_Calming&#39;, &#39;Traffic_Signal&#39;, &#39;Turning_Loop&#39;, &#39;Sunrise_Sunset&#39;, &#39;Civil_Twilight&#39;, &#39;Nautical_Twilight&#39;, &#39;Astronomical_Twilight&#39;], dtype=&#39;object&#39;) . We would Analyze some of the columns because there are many columns which could be explored at a later stage. . City --&gt; The cities where the Accidents take place | Start_Time --&gt; The starting time of the accidents | Start_Lat --&gt; The starting latitude of the accidents | Start_Lng --&gt; The starting longitude of the accidents | State --&gt; The States in which the accidents happened | Temperature --&gt; The temperature when the accident happened | Weather Condition --&gt; The weather conditions when the accident happened | Visibility --&gt; The visibility distance | Preciptation --&gt; The precipitation record of the area | Severity --&gt; The rate of severity of the accident | CITY Column . df[&#39;City&#39;] # This is the syntax for getting a column from a DataFrame. It is called a Series and a collection of series makes up a DataFrame. . 0 Dublin 1 Dayton 2 Cincinnati 3 Cincinnati 4 Akron ... 1516059 Riverside 1516060 San Diego 1516061 Orange 1516062 Culver City 1516063 Highland Name: City, Length: 1516064, dtype: object . cities = df[&#39;City&#39;].unique() # Find unique cities in the DataFrame len(cities) # Find the number of Unique cities by taking the length of the list cities . 10658 . We should look at the Cities where a high number of accidents take place . cities_by_accidents = df[&#39;City&#39;].value_counts() # the value_counts() method gives the number of times a certain value appears in the Series Cities. cities_by_accidents . Los Angeles 39984 Miami 36233 Charlotte 22203 Houston 20843 Dallas 19497 ... Manzanita 1 West Brooklyn 1 Garfield Heights 1 Belding 1 American Fork-Pleasant Grove 1 Name: City, Length: 10657, dtype: int64 . Now we will visualize the Top 50 cities where the most accidents happened. . sns.set_style(&#39;darkgrid&#39;) plt.figure(figsize = (15,10)) # Sets the figure size of the plot plt.title(&#39;Top 50 cities with the most accidents&#39;, weight = &#39;bold&#39;) # Sets the title of the plot plt.xlabel(&#39;Number of Accidents&#39;,weight = &#39;bold&#39;) # Sets the x-axis label of the plot plt.ylabel(&#39;Cities&#39; ,weight = &#39;bold&#39;) # sets the y-axis label of the plot sns.barplot(x = cities_by_accidents[:50].values, y = cities_by_accidents[:50].index) # Plots the bargraph for the Data. The SNS is the alias for the Seaborn library for plotting plt.show() # used to disply the plot. This is optional as JUPYTER notebooks automatically renders the plot. . Insights . The cities of Los Angeles and Miami have the highest number of Accidents. | New York Though it is a bustling city has a dramatically lower accident rate | Now lets see the distribution of the Accidents of the top 50 cities . sns.displot(data = cities_by_accidents[:50], kind = &#39;kde&#39;) # The [:30] takes up the the values from index 0 - 29 and plots them for the plot. plt.show() . We can see in the above plot that the distribution of the accidents is skewed to the right and has a long tail. So we can containarize our accidents of the cities into low accident cities and high accident cities. . high_accident_cities = cities_by_accidents[cities_by_accidents &gt;= 1000] # number of accidents greater than 1000 low_accident_cities = cities_by_accidents[cities_by_accidents &lt;= 1000] # number of accidents lesser than 1000 print(f&#39;The percentage of high accident cities : {len(high_accident_cities)/len(cities) *100:.2f} %&#39;) print(f&#39;The percentage of low accident cities : {len(low_accident_cities)/len(cities)*100:.2f} %&#39;) . The percentage of high accident cities : 2.36 % The percentage of low accident cities : 97.64 % . Now we will plot the histograms of high_accident_cities and low_accident_cities to see how they fare up and we can also see their distributions . sns.displot(data = high_accident_cities, kind = &#39;hist&#39;, kde = True, log_scale = True,height = 6, aspect = 12/6).set(title = &#39;Distribution of high Accident cities&#39;) plt.xlabel(&#39;High accident cities&#39;, weight = &#39;bold&#39;) plt.ylabel(&#39;Count&#39;,weight = &#39;bold&#39;) plt.show() . sns.displot(data = low_accident_cities, kind = &#39;hist&#39;,kde = True, log_scale = True,height = 6, aspect = 12/6).set(title = &#39;Distribution of Low Accident cities&#39;) plt.xlabel(&#39;Low accident cities&#39;, weight = &#39;bold&#39;) plt.ylabel(&#39;Count&#39;,weight = &#39;bold&#39;) plt.show() . There are some NULL values which explain the missing buckets in the histogram. But I have tried to perform the analysis without modifying the data in any way. | We can see that the distribution of the low accident cities and high accident cities have a fairly Normal Distribution but the distribution is Not strictly normal. | . START_TIME COLUMN . The Start_Time column is a Numerical Column which has the Starting Time of every accident. . df[&#39;Start_Time&#39;] . 0 2016-02-08 00:37:08 1 2016-02-08 05:56:20 2 2016-02-08 06:15:39 3 2016-02-08 06:15:39 4 2016-02-08 06:51:45 ... 1516059 2019-08-23 18:03:25 1516060 2019-08-23 19:11:30 1516061 2019-08-23 19:00:21 1516062 2019-08-23 19:00:21 1516063 2019-08-23 18:52:06 Name: Start_Time, Length: 1516064, dtype: object . The dtype = Object shows us that the Start_Time column contains data which is of String type, as in Python obbjects are natively strings So we need to convert it into a Special data type called as Datetime. The Datetime format provides other helper methods for easy and fast manipulation of Dates and Time. . df[&#39;Start_Time&#39;] = pd.to_datetime(df[&#39;Start_Time&#39;]) # The to_datetime() function of Python Pandas converts other formats to Datetime format and then we are again assigning it to Start_Time column. . Now after converting it to datetime if we check the dtype we can see that it shows dtype = datetime64[ns] . df[&#39;Start_Time&#39;] . 0 2016-02-08 00:37:08 1 2016-02-08 05:56:20 2 2016-02-08 06:15:39 3 2016-02-08 06:15:39 4 2016-02-08 06:51:45 ... 1516059 2019-08-23 18:03:25 1516060 2019-08-23 19:11:30 1516061 2019-08-23 19:00:21 1516062 2019-08-23 19:00:21 1516063 2019-08-23 18:52:06 Name: Start_Time, Length: 1516064, dtype: datetime64[ns] . df[&#39;Start_Time&#39;][0] . Timestamp(&#39;2016-02-08 00:37:08&#39;) . Now we would see the 24 hour distribution of the accidents and for that we would need to plot the histogram for the column Start_Time . sns.displot(data = df[&#39;Start_Time&#39;].dt.hour, bins = 24,kde = True,stat = &#39;percent&#39;, height = 6, aspect= 12/6 ).set(title = &#39;Distribution of accidents by hour&#39;) plt.xlabel(&#39;Hours&#39;, weight = &#39;bold&#39;) plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]) plt.ylabel(&#39;Percentage of accidents&#39;,weight = &#39;bold&#39;) plt.show() . The plot can be understood by the way that the height of each Bar shows the percentage of each bin with respect to the overall 100 percent. For eg. the first bin of 0-1 is 3% of the overall percentage and so on. So 3% of accidents happen between 0 and 1 hour in a 24 hour clock. . Insights . A high percentage of accidents occur between 6 - 10 am. Probably people in a hurry to get to work. | Then the percentage of accidents drop between 11am - 3pm. | The highest percentage of accidents occur between 2pm - 7pm with the highest being recorded at 5pm. So probably people rushing to get home after work. | Now lets find out if the accidents have increased or decreased on a year by year basis. . df[&#39;Year&#39;] = df[&#39;Start_Time&#39;].dt.year year_df = df[&#39;Year&#39;].value_counts() plt.figure(figsize = (15,8)) plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;Number of Accidents&#39;, weight = &#39;bold&#39;) plt.title(&#39;Accidents by year&#39;, weight = &#39;bold&#39;) sns.barplot(x = year_df.index, y = year_df.values) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Accidents by year&#39;}, xlabel=&#39;Year&#39;, ylabel=&#39;Number of Accidents&#39;&gt; . Insights . The Accidents by the year follow an exponentially increasing trend. But this might be the case that the data for the year 2020 has been the most documented | Now lets see the distribution of accidents for the Days of the Week . sns.displot(data = df[&#39;Start_Time&#39;].dt.dayofweek, bins = 7,stat = &#39;percent&#39;, height = 6, aspect= 12/6 ).set(title = &#39;Distribution of accidents by Days of the Week&#39;) plt.xlabel(&#39;Days of the Week&#39;, weight = &#39;bold&#39;) &quot;&quot;&quot; {0:&#39;Monday&#39;, 1:&#39;Tuesday&#39;, 2:&#39;Wednesday&#39;, 3:&#39;Thursday&#39;,4:&#39;Friday&#39;,5:&#39;Saturday&#39;,6:&#39;Sunday&#39;} &quot;&quot;&quot; plt.xticks([0,1,2,3,4,5,6]) plt.ylabel(&#39;Percentage of accidents&#39;,weight = &#39;bold&#39;) plt.show() . Insights . On weekdays the number of accidents is significantly higher | On weekends the number of accidents is lower. | Now lets see the distribution of accidents on Saturday&#39;s and Sunday&#39;s to see if the weekends also follow the trend of weekdays and the majority of accidents happen at some peak hours or the accidents are distributed evenly over the day. . sundays = df[&#39;Start_Time&#39;][df[&#39;Start_Time&#39;].dt.dayofweek == 6] saturdays = df[&#39;Start_Time&#39;][df[&#39;Start_Time&#39;].dt.dayofweek == 5] mondays = df[&#39;Start_Time&#39;][df[&#39;Start_Time&#39;].dt.dayofweek == 0] . sundays . 158 2016-02-14 03:58:33 159 2016-02-14 05:26:58 160 2016-02-14 16:30:40 161 2016-02-14 16:38:40 162 2016-02-14 17:40:17 ... 1513810 2019-08-18 22:48:14 1513811 2019-08-18 23:24:10 1513925 2019-08-18 22:56:56 1513926 2019-08-18 22:56:56 1513965 2019-08-18 22:54:41 Name: Start_Time, Length: 123775, dtype: datetime64[ns] . sns.displot(data = saturdays.dt.hour, bins = 24,stat = &#39;percent&#39;, height = 6, aspect= 12/6 ).set(title = &#39;Distribution of accidents on a Saturday&#39;) plt.xlabel(&#39;Hours&#39;, weight = &#39;bold&#39;) plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]) plt.ylabel(&#39;Percentage of accidents&#39;,weight = &#39;bold&#39;) plt.show() . sns.displot(data = sundays.dt.hour, bins = 24,stat = &#39;percent&#39;, height = 6, aspect= 12/6 ).set(title = &#39;Distribution of accidents on a Sunday&#39;) plt.xlabel(&#39;Hours&#39;, weight = &#39;bold&#39;) plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]) plt.ylabel(&#39;Percentage of accidents&#39;,weight = &#39;bold&#39;) plt.show() . sns.displot(data = mondays.dt.hour, bins = 24,stat = &#39;percent&#39;, height = 6, aspect= 12/6 ).set(title = &#39;Distribution of accidents on a Monday&#39;) plt.xlabel(&#39;Hours&#39;, weight = &#39;bold&#39;) plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]) plt.ylabel(&#39;Percentage of accidents&#39;,weight = &#39;bold&#39;) plt.show() . Insights . On saturdays and sundays the accidents are more distributed and no peak hours are there. | On mondays the accident have a very dominant peak between 8 - 10 am and between 5-7 pm indicating that the hypothesis was correct as accidents happen at rush hour. | Lets see which months have a high number of accidents. . sns.displot(data = df[&#39;Start_Time&#39;].dt.month, bins = 12,stat = &#39;percent&#39;, height = 6, aspect= 12/6 ).set(title = &#39;Monthly distribution of accidents&#39;) plt.xlabel(&#39;Months&#39;, weight = &#39;bold&#39;) plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12]) plt.ylabel(&#39;Percentage of accidents&#39;,weight = &#39;bold&#39;) plt.show() . Insights . The months data shows that months between OCT - DEC have the most accidents. | The month of July and August has the least number accidents. | The data of accidents shows that the during the summer months there are less accidents but what needs to be taken into account that the accidents are the lowest in the Monsoon season. This needs to be explored further. | Lets try to understand is the monthly data is correct by first checking that the data for the each of the years individually. If the find the same distribution for all the years then there is evidence that the trend is good but if the distribution is not the same then we should just say that the data is incorrect. . First lets filter the dataframe Start_Time column with by years so that we can individual series of years which we will use to plot the graphs. . df_2016 = df[df.Start_Time.dt.year == 2016] df_2017 = df[df.Start_Time.dt.year == 2017] df_2018 = df[df.Start_Time.dt.year == 2018] df_2019 = df[df.Start_Time.dt.year == 2019] df_2020 = df[df.Start_Time.dt.year == 2020] . Now lets plot the accidents by individual years . fig, ax = plt.subplots(nrows = 2,ncols = 3, figsize = (20,10), squeeze = False) # This divides the figure into multiple subplots and each one of it can plot a different variable plt.subplots_adjust(hspace = 0.5, wspace = 0.5) # Adjusts the horizontal and the vertical space of the plot w.r.t the labels and sides of the figure sns.histplot(df_2016[&#39;Start_Time&#39;].dt.month, ax = ax[0][0],bins = 12) ax[0][0].set_title(&#39;2016&#39;,weight = &#39;bold&#39;) ax[0][0].set_xlabel(&#39;Months&#39;) ax[0][0].set_ylabel(&#39;Percentage of Accidents&#39;) sns.histplot(df_2017[&#39;Start_Time&#39;].dt.month, ax = ax[0][1],bins = 12, stat = &#39;percent&#39;)#.set(title = &#39;2017&#39;) ax[0][1].set_title(&#39;2017&#39;, weight = &#39;bold&#39;) ax[0][1].set_xlabel(&#39;Months&#39;) ax[0][1].set_ylabel(&#39;Percentage of Accidents&#39;) sns.histplot(df_2018[&#39;Start_Time&#39;].dt.month, ax = ax[0][2],bins = 12,stat = &#39;percent&#39;)#.set(title = &#39;2018&#39;) ax[0][2].set_title(&#39;2018&#39;, weight = &#39;bold&#39;) ax[0][2].set_xlabel(&#39;Months&#39;) ax[0][2].set_ylabel(&#39;Percentage of Accidents&#39;) sns.histplot(df_2019[&#39;Start_Time&#39;].dt.month, ax = ax[1][0],bins = 12,stat = &#39;percent&#39;)#.set(title = &#39;2019&#39;) ax[1][0].set_title(&#39;2019&#39;, weight = &#39;bold&#39;) ax[1][0].set_xlabel(&#39;Months&#39;) ax[1][0].set_ylabel(&#39;Percentage of Accidents&#39;) sns.histplot(df_2020[&#39;Start_Time&#39;].dt.month, ax = ax[1][1],bins = 12,stat = &#39;percent&#39;)#.set(title=&#39;2020&#39;) ax[1][1].set_title(&#39;2020&#39;, weight = &#39;bold&#39;) ax[1][1].set_xlabel(&#39;Months&#39;) ax[1][1].set_ylabel(&#39;Percentage of Accidents&#39;) # plt.subplot(1,2,2) # sns.displot(df[&#39;Start_Time&#39;].dt.month) plt.show() . Insights . The data shows that for the years 2016 and 2020 there is virtually no data. | Thus the Hypothesis that lesser accidents happen in the months of July and August is Wrong because there is practically no data to support our reasoning. | For the years of 2017, 2018, 2019 we can see that the distributions of accidents is pretty uniform for all the months. | . Start Latitude and Start Longitude . The start latitude and the Start Longitude columns in the DataFrame provide information about the geographic latitude and longitude for the accidents taking place. . df[&#39;Start_Lat&#39;].head() . 0 40.10891 1 39.86542 2 39.10266 3 39.10148 4 41.06213 Name: Start_Lat, dtype: float64 . df[&#39;Start_Lng&#39;].head() . 0 -83.09286 1 -84.06280 2 -84.52468 3 -84.52341 4 -81.53784 Name: Start_Lng, dtype: float64 . We could plot a scatterplot of the Start latitude and the Start Longitude to see the density of the accidents. For simplicity of the visualization we would only plot a 0.001 percent sample of the data. . plt.figure(figsize = (10,8)) plt.title(&#39;Density of the accidents by latitude and longitude&#39;) sns.scatterplot(x = df.Start_Lng, y = df.Start_Lat,size = 0.001) plt.show() . The above figure shows us the density of accidents by their geographic latitude and longitude and these points in the scatter plot form a map of the United States of America. . We could also plot a HeatMap of the accidents using a Pandas library called as Folium. Folium provides a quick and easy setup for Geographic Maps . To plot Geographic Maps with Folium there are some steps that we need to follow . Create a Base Map of object of the region that we are trying to display. | Then we need to provide the Marker object with the data that we want to put on the Map. | The next step is to add the marker to the Base Map. | Create a Base Map with folium.Map(). The location paramters can be adjusted to get different regions of the world map and the zoom_start could be set to to prezoom to a particular are when the map loads. . Map = folium.Map(location = [30, -100], zoom_start = 4.3) . Now we would create the markers from the data. What we would so is that we would zip the latitude and the longitude column into a list of tuples and pass each (lat, lng) tuple to the Marker() method and then what we would do is that we would add the Markers object to the Map. . for lat, lon in list(zip(df.Start_Lat.sample(1000), df.Start_Lng.sample(1000))): # Here we would just zip 1000 data pointsof latitude and longitude marker = folium.Marker((lat, lon)) marker.add_to(Map) . Now lets visualize the Map that we have plotted . Map . Insights . We can see that most of the accidents in this sample have happened at the coasts while the inner parts of US have a slighlty lesser number of accidents. This is explainable because about 30% of the US population lives along the coast. | . State Column . The state column provide us information about the states in which the accidents happened. . The plotly package for plotting Data . import plotly.express as px - The plotly.express package provides a set of operations to create graphs faster without much hassle. import plotly.graph_objects as go - the graph_object is used to create more nuanced graphs where more low level features of the graph can be customised. . Choropleth Maps with Plotly . Choropleth Maps are types of Maps which have a predefined area is colored or patterned with respect to a statistical variable. For eg. States of a country shaded according to their respective population stats. Here we will plot a choropleth Map of US States on the basis of their accident count. | For plotting Choropleth Maps what we would need to import the nescessary libraries of plotly because plotly provides very easy plotting of interactive plots with a wide variety of formats which can be readily used in HTML pages for building dashboards. | . For plotting interactive maps first we need to make a dataframe of the states and their respective accident counts. The below DataFrame states_df provides us with state names and respective state abbreviations which would be used to label the various states in the map. . states_and_abbrevations_df = pd.read_csv(&#39;https://raw.githubusercontent.com/jasonong/List-of-US-States/master/states.csv&#39;) # This data is downloaded from a github repository of Jason Nong states_and_abbrevations_df.head() . We will take the State column of our accidents dataframe and perform the following operations . Get the value counts of accidents of the individual states. | convert them into a DataFrame for merging it with the States and Abbrevations DataFrame. | rest the index of the DataFrame and remove the index column as resetting the index causes the index to be converted to a column in the DataFrame by the name of Index. | . accidents_by_state = df[&#39;State&#39;].value_counts() # take the value counts accidents of individual states accidents_by_state_df = pd.DataFrame({&#39;Abbreviation&#39;:accidents_by_state.index, &#39;count&#39;: accidents_by_state.values}) # convert it to a dataframe for merging accidents_by_state_df.reset_index().drop(columns = &#39;index&#39;, inplace = True) # remove the index and then drop the index column. accidents_by_state_df.head() . Merge the states_and_abbrevations_df with the accidents_by_states_df to create an accidents_df which has the State name, State Abbrevations and count of the accidents. . accidents_df = pd.merge(states_and_abbrevations_df, accidents_by_state_df, how = &#39;inner&#39;, on = &#39;Abbreviation&#39;) accidents_df.head() . The Choropleth Map in plotly has roughly 2 parts. . The data of the Map | The layout of the Map | These parts are described standalone as Python dictionaries(key/value pairs) and then they are passed to the Figure object to generate the Plot and displayed by the fig.show() method. . data = dict(type = &#39;choropleth&#39;, # Type of graph that is to plotted locationmode = &#39;USA-states&#39;, # The geographic base-figure which could be generated with partitions of states. locations = accidents_df[&#39;Abbreviation&#39;], # Array like data structure comprising of the locations of the partitions in accordance with the data (locating the states in a US map). colorscale = &#39;teal&#39;, # Color scheme to be used. hovertext = accidents_df[&#39;State&#39;], # Text which would be displayed when hovering above a specific point on the plot (number of accidents and State Name). z = accidents_df[&#39;count&#39;], # Data values which would decide the shade of the color schem. In this context the number of the accidents by state. colorbar = {&#39;title&#39;: &#39;Accidents(High to Low)&#39;}) # Colorbar which would be displayed on the right hand side for understanding the graph. layout = dict(geo ={&#39;scope&#39;:&#39;usa&#39;}) # The layout of the Map fig = go.Figure(data = [data],layout = layout ) # Putting both the data and the layout in the figure to plot the map. # To customise the layout according to our preferences fig.update_layout( width = 1300, height = 500, margin = dict(r = 100), # r indicates distance of the plot from the right of the figure title_text = &#39;US Accidents by State&#39;, geo_scope=&#39;usa&#39;,) # Displaying the plot fig.show() . Insights . The State of California has the largest number of accidents follwed by Florida. | . Temperature Column . The Temperture Column in the DataFrame provide information about the Temperature at which the accidents happend. . df[&#39;Temperature(F)&#39;] . The Temperature is in Fahrenheit so we could convert it into Celsius and also change the Name of the column by removing the F. . df.rename(columns = {&quot;Temperature(F)&quot;:&#39;Temperature&#39;}, inplace = True) # Renaming a column df[&#39;Temperature&#39;] . Now we would convert the Temperature to celsius by defining a function that converts the Fahrenheit values to Celsius. . def func1(x): &quot;&quot;&quot; This function converts from Fahrenheit to Celsius &quot;&quot;&quot; if pd.isna(x) == False: # Checks that the value is Not Null return int((x-32)*(5/9)) # Converts the value to celsius and it also converts it into integer datatype and return it else: return x . df[&#39;Temperature&#39;] = df[&#39;Temperature&#39;].apply(func1) # apllying thr function to the Column by Series.apply() method . df[&#39;Temperature&#39;].head() # We can see that the values are now in Celsius. . x = df[&#39;Temperature&#39;].value_counts() # The value_counts() method provides us with the sum of all occurence of a particular value x . We would plot the occurences of for the first 40 temperatures and the number of accidents associated with them . b = x.head(40) # The DataFrame.head(n) displays the first n rows of the DataFrame plt.figure(figsize = (15,10)) plt.xticks(rotation = 90) plt.ylabel(&#39;Percentage of Accidents&#39;,weight = &#39;bold&#39;) plt.xlabel(&#39;Temperature&#39;,weight = &#39;bold&#39;) plt.title(&#39;Percentage of Accdidents by Temperature&#39;,weight = &#39;bold&#39;) sns.barplot(x = b.index, y = b.values*100/len(df[&#39;Temperature&#39;])) plt.show() . Insights . The above plot gives us the insight that a large percentage of accidents happen at 17&deg;C. | There is an unusual spike at 0&deg;C which could be worth investigating becuse it may be due to Snow as the winds become chilly. | We could explore the weather conditions and the Temperature column to see the distribution of of the accidents that happen at 0&deg;C . zero_degree_accidents_with_snow = df[(df[&#39;Temperature&#39;] == 0.0) &amp; (df[&#39;Weather_Condition&#39;] == &#39;Snow&#39;)] # Filtering the DataFrame print(f&quot;Number of accidents at Zero degree with Snowy weather Conditions : {zero_degree_accidents_with_snow.shape[0]}&quot;) . As this is is a very small Number of accidents with respect to the size of the dataset which is 1.5 Million records we should see all the weather conditions having 500 or more accidents at 0&deg;C. . zero_degree = df[df[&#39;Temperature&#39;] == 0.0] # Filtering the Tenperature Column by 0 degree celsius zero_degree = zero_degree[&#39;Weather_Condition&#39;].value_counts() # Taking the count of the unique weather conditions of the accidents at 0 degree celsius zero_degree_accidents = zero_degree[zero_degree.values&gt;=500] # Getting the weather conditions where the number of accidents is &gt;= 500 at 0 degree celsius. plt.figure(figsize = (20,10)) plt.title(&#39;Weather Conditions with 500 or more accidents at 0 degree Celsius&#39;) plt.ylabel(f&#39;Number of Accidents&#39;) plt.xlabel(&#39;Weather Conditions&#39;) sns.barplot(x = zero_degree_accidents.index, y = zero_degree_accidents.values) # Plotting the values plt.show() . Insights . The above plot shows that most of the accidents at 0&deg;C happen at Fair weather conditions. | . Visbility Column . The visibility Column provides information of the visbility in Miles at which the accidents happend. . We can ask a question that do people drive more carefully if the visibility is Low or do more accidents happen at low visibility conditions . df[&#39;Visibility(mi)&#39;] . visibility_value_counts = df[&#39;Visibility(mi)&#39;].value_counts() visibility_value_counts . plt.figure(figsize = (20,10)) plt.xticks(rotation = 90) plt.xlabel(&#39;Visibility in Miles&#39;) plt.ylabel(&#39;Number of Accidents&#39;) plt.title(&#39;Accidents VS Visibilty&#39;) sns.barplot(x = visibility_value_counts.index, y = visibility_value_counts.values) plt.show() . Insight . The plot above shows that most of the accidents happen at a Visibility of 10 miles and very few accidents happen at low visibility conditions. | But we should also plot very low visibilty because from the above plot above we can see that there are no accidents in very low visibility (visibility &lt;= 2 miles) conditions and see if it yeilds some more insights. | vis = df[df[&#39;Visibility(mi)&#39;] &lt; 2.0] plt.figure(figsize = (20,10)) plt.xticks(rotation = 90) plt.xlabel(&#39;Visbility in Miles&#39;) plt.ylabel(&#39;Number of Accidents&#39;) sns.barplot(x = vis[&#39;Visibility(mi)&#39;].value_counts().index, y = vis[&#39;Visibility(mi)&#39;].value_counts().values) . The plot above shows that most of the accidents happen at a Visibility of 10 miles and very few accidents happen at low visibility conditions. . But we should also plot very low visibilty because from the above plot above we can see that there are no accidents in very low visibility (visibility &lt;= 2 miles) conditions and see if it yeilds some more insights.nsights . We can see that that accidents do happen at low visibility conditions but the rate is lower so we can safely say that our Hypothesis that people drive more carefully during low visibilty conditions is correct. | . Amenities Column . The Amenities column is a Boolean Column which provides us with information about amenities such as hotels and roadside shops and diners. . df[&#39;Amenity&#39;].value_counts() . Insights . Amenity is a Boolean Column which shows that most of the roads did not have amenities. So the drivers were probably deprived of water and other things maybe food which would lead to more accidents. . . Precipitation Column . The precipitation column information about the rainfall in inches at the time the accident was recorded. . df[&#39;Precipitation(in)&#39;].value_counts() . light = df[df[&#39;Precipitation(in)&#39;] &lt;0.10] plt.figure(figsize = (22,10)) plt.xticks(rotation = 90) plt.title(&#39;Accidents in Light rainfall(less than 0.10 inches)&#39;, weight = &#39;bold&#39;) plt.xlabel(&#39;Rainfall in inches&#39;, weight = &#39;bold&#39;) plt.ylabel(&#39;Number of Accidents&#39;, weight = &#39;bold&#39;) sns.barplot(x = light[&#39;Precipitation(in)&#39;].value_counts().index, y = light[&#39;Precipitation(in)&#39;].value_counts().values ) plt.show() . Insights . We can see that the most of the accidents do not have a preciptation associated with them. | We will see how many accidents happend at 0.30 between 2.0 inches per hour which is the category of Heavy rainfall. . heavy = df[(df[&#39;Precipitation(in)&#39;] &gt;0.30) &amp; (df[&#39;Precipitation(in)&#39;] &lt;= 2.0)] plt.figure(figsize = (22,10)) plt.xticks(rotation = 90) plt.title(&#39;Accidents in Heavy rainfall(0.30 in - 2.0 in)&#39;, weight = &#39;bold&#39;) plt.xlabel(&#39;Rainfall in inches&#39;, weight = &#39;bold&#39;) plt.ylabel(&#39;Number of Accidents&#39;, weight = &#39;bold&#39;) sns.barplot(x = heavy[&#39;Precipitation(in)&#39;].value_counts().index, y = heavy[&#39;Precipitation(in)&#39;].value_counts().values ) plt.show() . Number of accidents in Moderate rainfall (0.10 - 0.30) inches . heavy = df[(df[&#39;Precipitation(in)&#39;] &gt;=0.10) &amp; (df[&#39;Precipitation(in)&#39;] &lt;= 0.30)] plt.figure(figsize = (22,10)) plt.xticks(rotation = 90) plt.title(&#39;Accidents in Moderate rainfall(0.10 in - 0.30 in)&#39;, weight = &#39;bold&#39;) plt.xlabel(&#39;Rainfall in inches&#39;, weight = &#39;bold&#39;) plt.ylabel(&#39;Number of Accidents&#39;, weight = &#39;bold&#39;) sns.barplot(x = heavy[&#39;Precipitation(in)&#39;].value_counts().index, y = heavy[&#39;Precipitation(in)&#39;].value_counts().values ) plt.show() . Insights . We see that most of the accidents happen in moderate rainfall i.e not too heavy and not too light but most of the accidents have no direct correlation with rainfall i.e they do not happen when its raining. | . Severity Column . The Severity column provides the data of the severity of the accidents. . df[&#39;Severity&#39;].value_counts() . The Sevrity column is a categorical column with severities from 1 to 4 (1 being least and 4 being the highest). . Here we will analyze the severity column with precipitation and weather conditions and find out that most severe accidents happen at what precipitation, Temperature and weather conditions. . severity_df = df[[&#39;Severity&#39;,&#39;Precipitation(in)&#39;,&#39;Weather_Condition&#39;, &#39;Temperature&#39;]] # Taking out the columns into a new Smaller DataFrame severity_df . Now we will group the severity dataframe in to groups by the severity categories. . severity_df = severity_df.groupby(by= &#39;Severity&#39;, as_index = True) # this creates a groupby object with the severity column as index severity_df # When we try to display the Groupby object by calling the object we get the memory adress at which the object is located. . severity_4_df = severity_df.get_group(4) # Getting the group of Severity = 4 from the groupby object severity_4_df . As we can see that all the entries in the DataFrame are for severity = 4. . severity_4_df[&#39;Weather_Condition&#39;].value_counts() # getting the value counts of the individual weather condition accident counts. . Now we will plot the data for the weather conditions where the accident count is greater than 500 and Severity = 4. . plt.figure(figsize = (20,10)) plt.title(&#39;Weather conditions of the Most Severe (Severity = 4) accidents&#39;, weight= &#39;bold&#39;) plt.ylabel(&#39;Number of Accidents of 4 severity&#39;, weight= &#39;bold&#39;) plt.xlabel(&#39;Weather Conditions&#39;, weight= &#39;bold&#39;) plt.xticks(rotation = 90) severe_weather = severity_4_df[&#39;Weather_Condition&#39;].value_counts() severe_weather_accidents = severe_weather[severe_weather.values &gt;=500] sns.barplot(x = severe_weather_accidents.index, y = severe_weather_accidents.values) plt.show() . Insights . By Analyzing the plot above we can say that the largest number of accidents of Sverity = 4 accidents happen in Clear Conditions | lets plot the Weather Conditions of Severity = 3 accidents . severity_3_df = severity_df.get_group(3) plt.figure(figsize = (20,10)) plt.title(&#39;Weather conditions of the accidents with severity 3&#39;,weight = &#39;bold&#39;) plt.ylabel(&#39;Number of Accidents&#39;,weight = &#39;bold&#39;) plt.xlabel(&#39;Weather Conditions&#39;,weight = &#39;bold&#39;) plt.xticks(rotation = 90) severe_weather = severity_3_df[&#39;Weather_Condition&#39;].value_counts() severe_weather_accidents = severe_weather[severe_weather.values &gt;=500] sns.barplot(x = severe_weather_accidents.index, y = severe_weather_accidents.values) plt.show() . Insights . Here also the most accidents with severity = 3 happen under clear conditions. | severity_2_df = severity_df.get_group(2) plt.figure(figsize = (20,10)) plt.title(&#39;Weather conditions of the accidents with severity 2&#39;,weight = &#39;bold&#39;) plt.ylabel(&#39;Number of Accidents&#39;,weight = &#39;bold&#39;) plt.xlabel(&#39;Weather Conditions&#39;,weight = &#39;bold&#39;) plt.xticks(rotation = 90) severe_weather = severity_2_df[&#39;Weather_Condition&#39;].value_counts() severe_weather_accidents = severe_weather[severe_weather.values &gt;=500] sns.barplot(x = severe_weather_accidents.index, y = severe_weather_accidents.values) plt.show() . Insights . The most of the Moderately Severe accidents (severity = 2) happen under Fair conditions | severity_1_df = severity_df.get_group(1) plt.figure(figsize = (20,10)) plt.title(&#39;Weather conditions of the accidents with severity 1&#39;,weight = &#39;bold&#39;) plt.ylabel(&#39;Number of Accidents&#39;,weight = &#39;bold&#39;) plt.xlabel(&#39;Weather Conditions&#39;,weight = &#39;bold&#39;) plt.xticks(rotation = 90) severe_weather = severity_1_df[&#39;Weather_Condition&#39;].value_counts() severe_weather_accidents = severe_weather[severe_weather.values &gt;=500] sns.barplot(x = severe_weather_accidents.index, y = severe_weather_accidents.values) plt.show() . Insights . The least severe accidents happen (Severity = 1) happen under Fair conditons. | Lets see at what precipitation do the most severe accidents happen. . severity_4_df = severity_df.get_group(4) plt.figure(figsize = (20,10)) plt.title(&#39;Precipitation of the accidents with severity 4&#39;) plt.ylabel(&#39;Number of Accidents&#39;) plt.xlabel(&#39;Precipitation&#39;) plt.xticks(rotation = 90) severe_weather = severity_4_df[&#39;Precipitation(in)&#39;].value_counts() severe_weather_accidents = severe_weather[severe_weather.values &gt;=500] sns.barplot(x = severe_weather_accidents.index, y = severe_weather_accidents.values) plt.show() . Insights . The most severe accidents happen when there is no precipitaion at all. | Lets see at what temperature the most severe accidents happen. . severity_4_df = severity_df.get_group(4) plt.figure(figsize = (20,10)) plt.title(&#39;Temperature of the accidents with severity 4&#39;) plt.ylabel(&#39;Number of Accidents&#39;) plt.xlabel(&#39;Temperature&#39;) plt.xticks(rotation = 90) severe_weather = severity_4_df[&#39;Temperature&#39;].value_counts() severe_weather_accidents = severe_weather[severe_weather.values &gt;=500] # sns.barplot(x = severe_weather.index, y = severe_weather.values) sns.barplot(x = severe_weather_accidents.index, y = severe_weather_accidents.values) plt.show() . Insights . The most severe accidents happen at 21&deg;C. | At what time of the day do the most severe accidents happen ? . severity_4_df = df.groupby(by = &#39;Severity&#39;).get_group(4) severity_by_hour = severity_4_df[&#39;Start_Time&#39;].dt.hour.value_counts() . plt.figure(figsize = (20,10)) plt.title(&#39;Number of Accidents of 4 Severity by Hour&#39;, weight = &#39;bold&#39;) plt.xlabel(&#39;Hour&#39;, weight = &#39;bold&#39;) plt.ylabel(&#39;Number of Accidents&#39;, weight = &#39;bold&#39;) plt.xticks(rotation = 90) sns.barplot(x = severity_by_hour.index , y = severity_by_hour.values) plt.show() . This shows that the most severe accidents happen between 3 pm - 6pm which is when people are coming home from work and between 7 am - 8 am which is people going to work. . Now lets see at what day do te most severe accidents happen? . severity_4_df = df.groupby(by = &#39;Severity&#39;).get_group(4) severity_by_day = severity_4_df[&#39;Start_Time&#39;].dt.weekday.value_counts() fig, ax = plt.subplots(figsize = (20,10)) plt.title(&#39;Number of Accidents of 4 Severity by days&#39;, weight = &#39;bold&#39;) plt.xlabel(&#39;Days&#39;) plt.ylabel(&#39;Number of Accidents of 4 Severity&#39;) &quot;&quot;&quot; {0:&#39;Monday&#39;, 1:&#39;Tuesday&#39;, 2:&#39;Wednesday&#39;, 3:&#39;Thursday&#39;,4:&#39;Friday&#39;,5:&#39;Saturday&#39;,6:&#39;Sunday&#39;} &quot;&quot;&quot; sns.barplot(x = severity_by_day.index , y = severity_by_day.values) plt.show() . Insights . Most Severe accidents happen on the weekdays and they are distributed evenly on the weekdays. | The weekends see a fewer number of High severity accidents. | Lets see if there is any particular city where the most severe accidents happen. . severe_accidents = df.groupby(by=&#39;Severity&#39;).get_group(4) severe_by_city = severe_accidents[&#39;City&#39;].value_counts() severe_by_city = severe_by_city[severe_by_city.values &gt;= 500] plt.figure(figsize = (20,10)) plt.xticks(rotation = 90) plt.xlabel(&#39;Cities&#39;, weight = &#39;bold&#39;) plt.ylabel(&#39;Number of Severe Accidents&#39;, weight = &#39;bold&#39;) plt.title(&#39;Most severe accidents by City&#39;, weight = &#39;bold&#39;) sns.barplot(x = severe_by_city.index, y = severe_by_city.values) plt.show() . Insights . The Cities of Miami and Atlanta have significantly higher severe accidents than the other cities. | Lets see the cities where mosst of the severity = 3 accidents happen. . severe_accidents = df.groupby(by=&#39;Severity&#39;).get_group(3) severe_by_city = severe_accidents[&#39;City&#39;].value_counts() severe_by_city = severe_by_city[severe_by_city.values &gt;= 500] plt.figure(figsize = (20,10)) plt.xticks(rotation = 90) plt.xlabel(&#39;Cities&#39;) plt.ylabel(&#39;Number of Severe Accidents&#39;) plt.title(&#39;Most severe (severity = 3) accidents by City&#39;) sns.barplot(x = severe_by_city.index, y = severe_by_city.values) plt.show() . Insights . The cities of Chicago and Houston have the most of the severity = 3 accidents. | . Summary of Insights . City The cities of Los Angeles and Miami have the highest number of Accidents. | New York Though it is a bustling city has a dramatically lower accident rate as compared to other cities. | . | Start_Time A high percentage of accidents occur between 6 - 10 am. Probably people in a hurry to get to work. | Then the percentage of accidents drop between 11am - 3pm. | The highest percentage of accidents occur between 2pm - 7pm with the highest being recorded at 5pm. So probably people rushing to get home after work. | The Accidents by the year follow an exponentially increasing trend. But this might be because case that the data for the year 2020 has been the most documented. | On weekdays the number of accidents is significantly higher. | On weekends the number of accidents is lower. | On saturdays and sundays the accidents are more distributed and no peak hours are there. | On mondays the accident have a very dominant peak between 8 - 10 am and between 5-7 pm indicating that the hypothesis was correct as more accidents happen at rush hour. | The months data shows that months between OCT - DEC have the most accidents. | The month of July and August has the least number accidents. | The data shows that for the years 2016 and 2020 there is virtually no data for July and August months. | Thus the Hypothesis that lesser accidents happen in the months of July and August is Wrong because there is practically no data to support our reasoning. | For the years of 2017, 2018, 2019 we can see that the distributions of accidents is pretty uniform for all the months. | . | Start_Latitude and Start_longitude We can see that most of the accidents in this sample have happened at the coasts while the inner parts of US have a slighlty lesser number of accidents. This is explainable because about 30% of the US population lives along the coast. | . | State The State of California has the largest number of accidents. | . | Temperature A large percentage of accidents happen at 17Â°C. | There is an unusual spike at 0Â°C which could be worth investigating becuse it may be due to Snow as the winds become chilly. | The plots show that most of the accidents at 0Â°C happen at Fair weather conditions. | . | Visibility The plots show that most of the accidents happen at a Visibility of 10 miles and very few accidents happen at low visibility conditions. | We can see that that accidents do happen at low visibility conditions (visibility &lt;= 2 miles) but the rate is lower so we can safely say that our Hypothesis that people drive more carefully during low visibilty conditions is correct. | . | Amenities Amenity is a Boolean Column which shows that most of places where the accidents happened did not have amenities nearby. So the drivers were probably deprived of food, water and other things which could have lead to more accidents. | . | Precipitation Most of the accidents do not have a preciptation associated with them. | For the accidents which do happen in rainfall most of the accidents happen under moderate rainfall conditions. | Very few accidents happen under heavy rainfall conditions. | . | Severity Column Most Severe accidents of (Severity = 4) happen in Clear Weather Conditions. | The most accidents with (Severity = 3) happen under Clear Weather conditions. | The most of the Moderately Severe accidents (Severity = 2) happen under Fair Weather conditions. | The least severe accidents happen (Severity = 1) happen under Fair Weather conditons. | The most severe accidents happen when there is no precipitaion at all. | The most severe accidents happen at 21Â°C. | This shows that the most severe accidents happen between 3 pm - 6pm which is when people are coming home from work and between 7 am - 8 am which is people going to work. | Most Severe accidents happen on the weekdays and they are distributed evenly on the weekdays. The weekends see a fewer number of High severity accidents. | The Cities of Miami and Atlanta have significantly higher severe accidents than the other cities. | The cities of Chicago and Houston have the most of the severity = 3 accidents. | . | Future Work . This by no means is a Comprehensive analysis of the dataset. I have done the analysis of the columns which interested me the most and people can download the dataset and experiment with their own columns and they are more than welcome to further my analysis. . Some of the Columns that could be explored are :- Street on which the accident happened. | Do a NLP analysis on the Description Column. | Side of the street where the accident happened. | Humidity and Wind Chill and Pressure columns to unearth any weather phenomenon on which the accidents happened like a Tornado. | Roundabout and Traffic Signal columns. | . | . References . Below provided are the links to the documentations for the libraries used . Python - https://docs.python.org/3/ | Pandas - https://pandas.pydata.org/docs/ | Numpy - https://numpy.org/doc/ | Matplotlib - https://matplotlib.org/stable/ | Seaborn - https://seaborn.pydata.org/index.html | Folium - https://python-visualization.github.io/folium/ | US Accidents Dataset - https://www.kaggle.com/sobhanmoosavi/us-accidents | Accident Image url - https://in.pinterest.com/pin/647533252665093717/ | Thankyou . I would like to thank the reader for taking out time to read this Analysis. Questions, Comments and Suggestions on improving are welcome and greatly appreciated. With your support I will try to build better reports in the future and provide you with better insights. .",
            "url": "https://anantinfinity9796.github.io/Paradox-AI-blog/pandas/numpy/matplotlib/seaborn/folium/plotly/2022/03/07/US_Accidents_Analysis.html",
            "relUrl": "/pandas/numpy/matplotlib/seaborn/folium/plotly/2022/03/07/US_Accidents_Analysis.html",
            "date": " â€¢ Mar 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://anantinfinity9796.github.io/Paradox-AI-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â€¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://anantinfinity9796.github.io/Paradox-AI-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Anant Shukla and I am a Software Engineer trying to learn more about AI and sharing what I have learned through this blog is my way of giving back to the community so that it could help someone just starting into AI. .",
          "url": "https://anantinfinity9796.github.io/Paradox-AI-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://anantinfinity9796.github.io/Paradox-AI-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}